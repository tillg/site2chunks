# Dynamic Page Scraping - Single Page Applications (SPAs)

**Problem Statement:** The current scraper (`scrape.py`) cannot handle websites that use JavaScript for navigation instead of traditional HTML links. Example: https://www.mgm-tp.com/a12.htmlshowcase/#/

**Solution:** Implement Playwright-based dynamic scraper with pluggable strategy pattern.

**Date:** 2025-01-15
**Status:** Approved - Implementation Ready
**Decision:** Use Playwright for headless browser scraping

---

## TL;DR

**Problem:** SPAs with JavaScript navigation can't be scraped with current HTML-based approach.

**Solution:** Add Playwright-based dynamic scraper with strategy pattern.

**Implementation:**
1. Install: `pip install playwright && playwright install chromium`
2. Create: `scraper_dynamic.py` module
3. Configure: Add `strategy: "dynamic"` to config.yaml
4. Use: `python3 scrape.py a12_widgets` (with dynamic strategy)

**Timeline:** 2-3 weeks for full implementation

---

## Problem Analysis

### Current Scraper Limitations

The existing scraper has these characteristics:

1. **HTML-based link extraction** (`extract_urls_from_html()` in scrape.py:240)
   - Uses BeautifulSoup to find `<a href="">` tags
   - Cannot discover URLs generated by JavaScript
   - Cannot interact with buttons, dropdowns, or dynamic menus

2. **Static page fetching** (requests library)
   - Fetches initial HTML only
   - Does not execute JavaScript
   - Cannot wait for dynamic content to load
   - Cannot interact with the page

3. **Same-domain crawling**
   - Relies on extracting links from HTML
   - Works well for traditional multi-page websites
   - Fails for Single Page Applications (SPAs)

### Single Page Application Characteristics

**What is an SPA?**
- Single HTML page loaded initially
- JavaScript dynamically updates content
- Navigation via hash routing (`#/page`) or History API
- Content rendered client-side
- Traditional links replaced with JavaScript event handlers

**Example: mgm-tp.com/a12.htmlshowcase**
- Hash-based routing: `#/`
- Navigation via JavaScript buttons/events
- Content loaded/rendered dynamically
- No traditional `<a href="">` links to discover

**Other Common SPA Patterns:**
- React Router: `example.com/about` (uses History API)
- Vue Router: Hash mode `#/about` or History mode
- Angular: Similar patterns
- Frameworks: React, Vue, Angular, Svelte, etc.

---

## Chosen Solution: Playwright-Based Dynamic Scraper

### Why Playwright?

**Decision Rationale:**

Based on the requirements and trade-off analysis, **Playwright** is the chosen solution for the following reasons:

1. **Modern and Well-Maintained:** Active development, great documentation
2. **Superior Performance:** Faster than Selenium, efficient resource usage
3. **Built-in Waiting:** Smart auto-wait reduces flaky tests
4. **Multi-Browser Support:** Chromium, Firefox, WebKit
5. **Python-First Design:** Native async/sync APIs for Python
6. **Developer Experience:** Excellent debugging tools and error messages

**Technology:** Playwright (Python)

**How it works:**
- Launches a real browser (Chrome/Firefox) in headless mode
- Executes JavaScript like a real user
- Can interact with buttons, forms, dropdowns
- Can wait for dynamic content to load
- Can scroll to trigger lazy loading
- Can handle hash routing and History API

**Advantages:**
- ✅ Handles all JavaScript execution
- ✅ Can interact with complex UIs
- ✅ Discovers dynamically generated URLs
- ✅ Handles SPAs, AJAX, and dynamic content
- ✅ Can take screenshots for verification
- ✅ Most reliable for modern web apps

**Trade-offs (Accepted):**
- ⚠️ Slower than requests (browser overhead) - **Acceptable** for comprehensive content extraction
- ⚠️ Higher resource usage (RAM, CPU) - **Not a concern** with modern hardware
- ⚠️ Requires browser installation - **Automated** with `playwright install chromium`
- ⚠️ More complex error handling - **Mitigated** by good abstractions
- ⚠️ Potential for detection/blocking - **Manageable** with proper headers and delays

**Implementation Complexity:** Medium

**Use Cases:**
- SPAs with JavaScript navigation
- Pages with authentication flows
- Dynamic content loading
- Infinite scroll pages
- Complex interaction patterns

---

## Installation & Setup

### Install Playwright

```bash
# Activate virtual environment
source .venv/bin/activate

# Install Playwright package
pip install playwright

# Install Chromium browser (required)
playwright install chromium

# Optional: Install other browsers
playwright install firefox
playwright install webkit
```

### Update Dependencies

Add to `.venv` requirements:

```bash
pip freeze > requirements.txt
```

Or manually add to `requirements.txt`:

```
playwright>=1.40.0
```

### Verify Installation

```bash
# Test Playwright installation
python3 -c "from playwright.sync_api import sync_playwright; print('✓ Playwright installed')"

# Check installed browsers
playwright --version
```

---

## Architecture Design

### Modular Scraper with Strategy Pattern

**Goal:** Extend the scraper to support both traditional and dynamic websites without breaking existing functionality.

### Component Overview

```
scrape.py (main entry point)
├── config_loader.py (existing - load config sets)
│
├── scraper_traditional.py (extract existing logic)
│   ├── requests + BeautifulSoup
│   └── HTML link extraction
│
└── scraper_dynamic.py (NEW - Playwright)
    ├── Browser automation (Chromium/Firefox)
    ├── JavaScript execution
    ├── Dynamic URL discovery
    └── Content extraction after JS rendering

Configuration (config.yaml):
  scraping:
    strategy: "traditional"  # or "dynamic"
    dynamic:
      headless: true
      timeout: 30000
      nav_selectors: [...]
```

---

## Implementation Plan

#### Phase 1: Add Playwright Support

**File:** `scraper_dynamic.py`

```python
"""
Dynamic scraper using Playwright for JavaScript-heavy sites.
"""

from playwright.sync_api import sync_playwright, Page
from pathlib import Path
from typing import List, Set
import time

class DynamicScraper:
    """Scraper for single-page applications and JavaScript-heavy sites."""

    def __init__(self, headless: bool = True, timeout: int = 30000):
        self.headless = headless
        self.timeout = timeout
        self.discovered_urls = set()

    def discover_spa_routes(self, base_url: str, patterns: List[str]) -> Set[str]:
        """
        Discover routes in a single-page application.

        Args:
            base_url: Base URL of the SPA
            patterns: List of selectors to click/interact with

        Returns:
            Set of discovered URLs
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            page = browser.new_page()

            # Navigate to base URL
            page.goto(base_url, wait_until='networkidle')

            # Discover routes by clicking navigation elements
            for pattern in patterns:
                self._click_and_discover(page, pattern)

            browser.close()

        return self.discovered_urls

    def _click_and_discover(self, page: Page, selector: str):
        """Click elements and record URL changes."""
        elements = page.query_selector_all(selector)

        for element in elements:
            try:
                # Get current URL
                before_url = page.url

                # Click element
                element.click()
                page.wait_for_load_state('networkidle', timeout=self.timeout)

                # Get new URL
                after_url = page.url

                # Record if URL changed
                if after_url != before_url:
                    self.discovered_urls.add(after_url)

                # Go back if needed
                if '#' in after_url or after_url != before_url:
                    page.go_back()
                    page.wait_for_load_state('networkidle')

            except Exception as e:
                print(f"Error clicking element: {e}")
                continue

    def scrape_with_js(self, url: str, output_dir: Path) -> Path:
        """
        Scrape a single URL with JavaScript execution.

        Returns path to saved markdown file.
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            page = browser.new_page()

            # Navigate and wait for content
            page.goto(url, wait_until='networkidle')

            # Extract content after JavaScript execution
            content = page.content()

            # Convert to markdown (reuse existing logic)
            from scrape import clean_filename, md
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(content, 'html.parser')

            # Remove script and style
            for script in soup(["script", "style"]):
                script.decompose()

            # Convert to markdown
            markdown_content = md(
                str(soup),
                heading_style="ATX",
                bullets="-",
                strip=["img", "svg"],
            )

            # Save to file
            output_path = output_dir / clean_filename(url)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(markdown_content, encoding='utf-8')

            browser.close()
            return output_path
```

#### Phase 2: Update Configuration Schema

**Add to config.yaml:**

```yaml
scraping:
  # Existing fields
  recursive: true
  max_hops: 2

  # New fields for dynamic scraping
  strategy: "traditional"  # "traditional" | "dynamic" | "hybrid"

  # Dynamic scraping settings
  dynamic:
    headless: true
    timeout: 30000  # milliseconds
    wait_for: "networkidle"  # "load" | "domcontentloaded" | "networkidle"

    # Selectors for navigation discovery
    nav_selectors:
      - "nav a"
      - "button[role='link']"
      - ".menu-item"

    # Hash route patterns (for SPAs)
    route_patterns:
      - "#/widgets/*"
      - "#/components/*"

    # JavaScript to execute for discovery
    discovery_script: |
      // Optional: custom JS to discover routes
      return Array.from(document.querySelectorAll('[data-route]'))
        .map(el => el.getAttribute('data-route'));
```

#### Phase 3: Integration with Existing Scraper

**Update scrape.py:**

```python
def main():
    # ... existing argument parsing ...

    # Determine scraping strategy
    strategy = scraping_config.get('strategy', 'traditional')

    if strategy == 'dynamic':
        # Use dynamic scraper
        from scraper_dynamic import DynamicScraper

        dynamic_config = scraping_config.get('dynamic', {})
        scraper = DynamicScraper(
            headless=dynamic_config.get('headless', True),
            timeout=dynamic_config.get('timeout', 30000)
        )

        # Discover routes
        nav_selectors = dynamic_config.get('nav_selectors', ['nav a', 'button'])
        discovered_urls = scraper.discover_spa_routes(base_url, nav_selectors)

        # Add discovered URLs to queue
        for url in discovered_urls:
            url_queue.add_url_to_scrape(url, hop=1)

    elif strategy == 'hybrid':
        # Use dynamic for discovery, traditional for scraping
        # ... hybrid implementation ...

    else:  # 'traditional'
        # Use existing scraper (current behavior)
        # ... existing implementation ...
```

---

## Dependencies

### For Playwright (Recommended)

```bash
pip install playwright
playwright install chromium  # Install browser
```

**Pros:**
- Modern, well-maintained
- Better API than Selenium
- Built-in waiting mechanisms
- Fast and reliable

### For Selenium (Alternative)

```bash
pip install selenium webdriver-manager
```

**Pros:**
- More mature
- Larger community
- More examples available

---

## Configuration Examples

### Example 1: mgm-tp.com (SPA with Hash Routing)

```yaml
config_set:
  name: a12_widgets
  description: "A12 Widgets Showcase (SPA)"

paths:
  data_dir: data/a12_widgets
  scrapes_dir: "{data_dir}/scrapes"
  cleaned_dir: "{data_dir}/cleaned"
  chunks_dir: "{data_dir}/chunks"
  urls_file: urls.txt

scraping:
  strategy: "dynamic"
  recursive: false  # Not needed - discover routes programmatically

  dynamic:
    headless: true
    timeout: 30000
    wait_for: "networkidle"

    # Navigation discovery
    nav_selectors:
      - "nav button"
      - ".widget-nav a"
      - "[role='navigation'] button"

    # Hash patterns to recognize
    route_patterns:
      - "#/*"

    # Custom discovery (optional)
    discovery_script: |
      // Extract all widget routes
      const widgets = window.__WIDGET_ROUTES__ || [];
      return widgets.map(w => `${location.origin}${location.pathname}#/${w}`);

  skip_patterns:
    - "*/login"
    - "*/admin/*"

# ... rest of config ...
```

### Example 2: Hybrid Approach

```yaml
scraping:
  strategy: "hybrid"

  # Use dynamic for discovery
  dynamic:
    headless: true
    discovery_only: true
    nav_selectors:
      - "nav a"
      - "button[onclick]"

  # Use traditional for actual scraping
  # (discovered URLs will be added to queue)
```

---

## Manual URL Discovery (Quickest Solution)

For the immediate case of mgm-tp.com/a12.htmlshowcase:

### Step 1: Manual Discovery

Open the site in a browser and:
1. Open DevTools → Network tab
2. Click through all navigation items
3. Record all URLs/hash routes

### Step 2: Create URL List

```bash
# config/a12_widgets/urls.txt
https://www.mgm-tp.com/a12.htmlshowcase/#/
https://www.mgm-tp.com/a12.htmlshowcase/#/widgets/button
https://www.mgm-tp.com/a12.htmlshowcase/#/widgets/card
https://www.mgm-tp.com/a12.htmlshowcase/#/widgets/dialog
# ... etc
```

### Step 3: Use Dynamic Scraper for Content

Even with manual URLs, you'll need JavaScript execution to get the content:

```bash
# Use dynamic strategy to execute JavaScript
python3 scrape.py a12_widgets --strategy dynamic
```

---

## Testing Strategy

### Phase 1: Manual Testing
1. Create a12_widgets config set with manual URLs
2. Test with simple Playwright script
3. Verify content extraction

### Phase 2: Automated Discovery
1. Implement route discovery
2. Test with mgm-tp.com
3. Compare manual vs automated results

### Phase 3: Integration
1. Add strategy flag to scraper
2. Test backward compatibility
3. Test with traditional sites

---

## Migration Path

### Short Term (Immediate Need)
1. **Manual URL list** for a12_widgets
2. **Simple Playwright script** for scraping those URLs
3. No changes to existing scraper

### Medium Term (1-2 weeks)
1. **Create scraper_dynamic.py** module
2. **Add strategy configuration** option
3. **Test with a12_widgets**

### Long Term (1-2 months)
1. **Full hybrid mode** implementation
2. **Route pattern matching**
3. **Framework-specific detection**
4. **Performance optimization**

---

## Cost-Benefit Analysis

| Approach | Implementation Time | Resource Usage | Reliability | Maintenance |
|----------|-------------------|----------------|-------------|-------------|
| **Manual URLs** | 30 minutes | Low | Medium | High (manual updates) |
| **Playwright (Full)** | 1-2 weeks | High | Very High | Low |
| **Hybrid** | 1 week | Medium | High | Low |
| **API Reverse** | Varies | Very Low | Medium | High (breaks on changes) |
| **Sitemap** | 1 hour | Low | Medium | Low |

---

## Recommendations

### For mgm-tp.com/a12.htmlshowcase (Immediate)

**Recommended: Hybrid Approach**

1. **Now (1 hour):**
   - Manually discover and list all widget URLs
   - Create urls.txt with all routes
   - Test with existing scraper (may need dynamic mode for content)

2. **This Week (3-5 hours):**
   - Create simple Playwright wrapper for content extraction
   - Add `--dynamic` flag to scraper
   - Test and validate output

3. **Next Sprint (1-2 weeks):**
   - Implement automatic route discovery
   - Add to config.yaml schema
   - Full integration with existing pipeline

### For Future SPAs

**Recommended: Full Dynamic Scraper Module**

- Invest in Playwright integration
- Support multiple strategies
- Make it configurable per config set
- Maintain backward compatibility

---

## Open Questions

1. **Browser choice?** Chromium (recommended), Firefox, or WebKit?
2. **JavaScript timeout defaults?** 30 seconds reasonable?
3. **Rate limiting?** Should we add delays for dynamic scraping?
4. **Screenshot capability?** Useful for debugging/validation?
5. **Authentication?** How to handle login-protected SPAs?
6. **Cost?** Headless browser scraping is resource-intensive - cloud vs local?

---

## References

- **Playwright Documentation:** https://playwright.dev/python/
- **Selenium Documentation:** https://selenium-python.readthedocs.io/
- **SPA Scraping Patterns:** https://github.com/topics/spa-scraping
- **Web Scraping Best Practices:** https://scrapinghub.com/guides

---

## Next Steps

**Decision Required:**
1. Choose approach for a12_widgets (immediate need)
2. Approve architecture for dynamic scraping (medium term)
3. Prioritize implementation (resource allocation)

**Recommended Action:**
- Start with **manual URL discovery** for a12_widgets (quick win)
- Plan **Playwright integration** for next sprint (sustainable solution)
- Create **proof-of-concept** script to validate approach

---

---

## Summary & Decision

### ✅ Chosen Solution: Playwright Dynamic Scraper

**Decision:** Implement Playwright-based dynamic scraper with strategy pattern integration.

**Rationale:**
- Most reliable solution for JavaScript-heavy SPAs
- Modern, well-maintained technology
- Good developer experience and debugging tools
- Acceptable performance trade-offs
- Supports both automatic discovery and manual URL lists

**Implementation Priority:**
1. **Phase 1** (Week 1): Create `scraper_dynamic.py` module
2. **Phase 2** (Week 1): Add configuration support
3. **Phase 3** (Week 2): Integrate with existing scraper
4. **Phase 4** (Week 2): Test with a12_widgets site
5. **Phase 5** (Week 3): Documentation and refinement

**Next Actions:**
- [ ] Install Playwright: `pip install playwright && playwright install chromium`
- [ ] Create `scraper_dynamic.py` following the implementation plan
- [ ] Update `config.yaml` schema to support `strategy: "dynamic"`
- [ ] Test with a12_widgets config set
- [ ] Document usage in CLAUDE.md

---

**Document Status:** Approved - Implementation Ready
**Last Updated:** 2025-01-15
**Next Review:** After Phase 1 implementation
